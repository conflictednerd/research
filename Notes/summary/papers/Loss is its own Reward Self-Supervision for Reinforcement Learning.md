# Loss is its own Reward: Self-Supervision for Reinforcement Learning

The paper can be found [here](https://arxiv.org/abs/1612.07307)

### Why?

A reinforcement learning agent has to simultaneously_ learn suitable representations_ for its observations and how to use these observations to _make decisions_ all the while its trying to _explore_ the environment. The only supervisory signal provided to the agent is the reward, which can be very sparse. For these reasons, it is notoriously difficult to train an RL agent, especially in complex environments where the sample efficiency becomes very low.

This paper proposes to augment a policy's loss function with self-supervisory losses which provide an instant feedback (contrary to sparse rewards) and can help the agent in learning useful representations.

### How?

Consider a neural network that acts as the policy of an RL agent. There are two ways to incorporate self supervisory losses into this network. We can either pre-train the network or use these new losses in an online manner.

For _pre-training_, we remove the value and policy heads (for actor critic RL) from the net and add a custom head for the self-supervised task. We then use a random policy to collect samples $(s, a, r, s')$ just like is done in normal RL, and then use these to train the network end-to-end. After this pre-training, we remove the SSL head and put back the value and policy head back in their place and train the agent to optimize the RL objective. The hope is that the pre-training can help provide representations that are useful and transferrable so that the agent can use them and train faster.

In _joint optimization_ of RL and SSL objectives, we use the samples generated by the policy and simply add up the RL and SSL loss to compute the gradient.

<img src="images\SSLLoss.jpg" alt="SSLLoss" style="zoom:67%;" />

#### SSL Tasks:

There are a number of SSL tasks that are proposed in the paper. All of these tasks use the normal transition data samples, $(s, a, r, s')$, and use an additional head on top of the main encoder part of the RL policy model to generate their predictions and respective losses.

1. **Reward:** Using the current state $s$, we can either predict the immediate reward, $r$ like a regression task, or bin the rewards and predict which category it belongs to. For instance, predict whether the next reward is positive, negative or 0. I think this will not be of much use in sparse reward environments.
2. **Dynamics:** This task uses the current and next states, $(s, s')$ and learns about the dynamics of the environment. Because $s$ maybe high-dimensional and noisy and hard to predict, instead of directly predicting $s'$ from $s$, we can use a _dynamic verification task_. In this task, we try to predict whether a pair $(s, s')$ has come from the environment or not. We can also include actions and make our prediction action-dependent. Another proposed way is to encode a series of consecutive observations, $o_1, \cdots, o_k$, where one observation is corrupted in that it belongs to another timestep. The SSL head has to predict which one of these observations is corrupted.
3. **Inverse Dynamics:** Here the task is to learn a mapping $\mathcal{S}\times \mathcal {S} \to \mathcal A$, which tries to predict which action was taken from two consecutive observations. For discrete actions, this reduces to a classification task and for continuous action spaces it becomes a regression task.
4. **Reconstruction:** Another SSL task is to reconstruct the observations from the encoded versions. This can be done by either adding a decoder (similar to a VAE) or by adding a full discriminator and decoder similar to bidirectional GANs. This method is probably not that beneficial because it does not care about the temporal information present in samples such as environment dynamics. Also, single observation may be noisy or high-dimensional, making it hard to generate them from a low-dimensional embedding.



### Future Work:

1. **More SSL tasks:** Especially when pre-training, we can use a full trajectory to learn representations. An idea for a task that uses a longer sequence of observations is to encode them using the shared encoder model to obtain a sequence of latent representations $z_1, z_2, \cdots, z_n$. We can then use these to predict which one had a larger/smaller reward, from which timestep (and to the end) we gathered the most discounted reward, etc.

2. **Expert pre-training:** It might be a good idea to use both expert demonstrations and random policy trajectories to generate samples for pre-training. This way our representations will (hopefully) be useful both in the beginning of RL training when our policy is nearly random as well as later on when our policy becomes more accurate.

3. **Goal conditioned and NL goals:** Another SSL task that can be useful is **predicting the mission from a successful trajectory (or portions of it)**. More formally, given the encoding of observations of a <u>successful</u> trajectory, $\tau_z = z_1, z_2, \cdots, z_n$ and the encoding of a set of missions $\{m_1, \cdots, m_k\}$, one of which is the encoding of the actual mission for this trajectory, namely $m^+$, the task is to determine the correct mission encoding. This can be done using similar ideas to CPC and noise contrastive loss. For instance, we can get a score for each mission by using a function $f(\tau_z, m)$ and use these as logits and then minimize the cross-entropy between this distribution and the correct-mission-delta-distribution. (This can _jointly learn a representation for the sentence and the observations_) (Maybe its better to _compare its results with imitation learning baselines_, because they will probably require a large number of expert demonstrations. In general, it seems to me that _SSL pre-training in language conditioned settings requires expert trajectories_ if it wishes to use both the observations and the NL prompt)

   The function $f$ should take as input an encoded trajectory $\tau_z$ and a mission $m$ and output a single positive score. It can be simple or complex. Some ideas are

   + $f(\tau_z, m) = \exp (\max_i [{z_i^TWm}]) = \max_i [ \exp (z_i^TWm)]$

     (This can also be relaxed to be the average of $k$ highest scores, not just the highest. Or we can use pairs or triplets of $z_i$s instead of only a single one)

   + We can use an attention based model where the query is $m$ and the key and value vectors are (linearly transformed) elements of $\tau_z$. In each attention head, we combine the value vectors, $v_i = W^{value}z_i$ according to how similar their key is to $m$. After we this, we will have one vector pair attention head: $h_1, \cdots, h_l$. We can now derive the final score using the relation below:
     $$
     f(\tau_z, m) = \exp {\left[ \sum_{i=1}^{l} h_i^TWm \right]} =^? \exp {\left[ \left(\sum_i h_i^T\right)Wm\right]}
     $$

     $$
     h = \sum_{i = 1}^{n} \sigma (z_i^T W^{key} m) [W^{value}z_i]
     $$

     
